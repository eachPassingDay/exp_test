import os
import cv2
import numpy as np
import open3d as o3d
import torch
from torch import nn

from colorama import Fore, Style
from torch.autograd import Variable
from dg_slam.gaussian.general_utils import inverse_sigmoid, build_rotation
from dg_slam.gaussian.common import (get_camera_from_tensor, get_samples, get_samples_with_pixel_grad, get_samples_point_add,
                        get_tensor_from_camera, setup_seed, random_select)
from dg_slam.gaussian.logger import Logger
from dg_slam.gaussian.loss_utils import ssim
from dg_slam.gaussian.gaussian_render import render
from dg_slam.gaussian.graphics_utils import getProjectionMatrix
from dg_slam.gaussian.common import focal2fov, convert3x4_4x4
from dg_slam.gaussian_model import GaussianModel
from dg_slam.warp.depth_warp import depth_warp_pixel

from skimage.color import rgb2gray
from skimage import filters
from scipy.interpolate import interp1d
import wandb

def pose_matrix_from_quaternion(pvec):
    """ convert 4x4 pose matrix to (t, q) """
    from scipy.spatial.transform import Rotation
    pose = np.eye(4)
    pose[:3, :3] = Rotation.from_quat(pvec[3:]).as_matrix()
    pose[:3, 3] = pvec[:3]
    return pose

class gs_tracking_mapping():
    def __init__(self, cfg, args, video):
        self.cfg = cfg
        self.args = args
        self.video = video
        self.scale_factor = 0.8

        self.low_gpu_mem = cfg['low_gpu_mem']
        self.verbose = cfg['verbose']
        self.dataset = cfg['dataset']
        self.time_string = cfg["data"]["exp_name"]
        self.output = os.path.join(cfg["data"]["output"], self.time_string)
        os.makedirs(self.output, exist_ok=True)
        self.ckptsdir = os.path.join(self.output, 'ckpts')
        # [æ–°å¢] åˆ›å»ºå‰ªæè°ƒè¯•ç›®å½• (Debug Output Directory)
        self.debug_prune_dir = os.path.join(self.output, 'debug_pruning')
        os.makedirs(self.debug_prune_dir, exist_ok=True)

        self.H, self.W, self.fx, self.fy, self.cx, self.cy = cfg['cam']['H'], cfg['cam'][
            'W'], cfg['cam']['fx'], cfg['cam']['fy'], cfg['cam']['cx'], cfg['cam']['cy']
        self.edge = None        
        self.update_scale()
        self.update_cam()

        self.gaussians = GaussianModel(cfg)

        self.n_img = cfg["data"]["n_img"]
        self.estimate_c2w_list = torch.zeros((self.n_img, 4, 4))
        self.gt_c2w_list = torch.zeros((self.n_img, 4, 4))
        self.idx = torch.zeros((1)).int()
        self.mapping_idx = torch.zeros((1)).int()
        # åœ¨ __init__ ä¸­æ·»åŠ 
        self.last_dynamic_ratio = 0.0
        self.last_keyframe_idx = -1

        self.wandb = cfg['wandb']
        self.project_name = cfg['project_name']
        self.use_dynamic_radius = cfg['use_dynamic_radius']
        self.dynamic_r_add, self.dynamic_r_query = None, None
        self.encode_exposure = cfg['model']['encode_exposure']
        self.radius_add_max = cfg['pointcloud']['radius_add_max']
        self.radius_add_min = cfg['pointcloud']['radius_add_min']
        self.radius_query_ratio = cfg['pointcloud']['radius_query_ratio']
        self.color_grad_threshold = cfg['pointcloud']['color_grad_threshold']
        self.eval_img = cfg['rendering']['eval_img']
        
        self.device = cfg['mapping']['device']
        self.fix_geo_decoder = cfg['mapping']['fix_geo_decoder']
        self.fix_color_decoder = cfg['mapping']['fix_color_decoder']
        self.eval_rec = cfg['meshing']['eval_rec']
        self.BA = cfg['mapping']['BA']
        self.BA_cam_lr = cfg['mapping']['BA_cam_lr']
        self.ckpt_freq = cfg['mapping']['ckpt_freq']
        self.mapping_pixels = cfg['mapping']['pixels'] 
        self.pixels_adding = cfg['mapping']['pixels_adding']
        self.pixels_based_on_color_grad = cfg['mapping']['pixels_based_on_color_grad']
        self.pixels_based_on_render = cfg['mapping']['pixels_based_on_render']
        self.add_pixel_depth_th = cfg['mapping']['add_pixel_depth_th']
        self.num_joint_iters = cfg['mapping']['iters']
        self.geo_iter_first = cfg['mapping']['geo_iter_first']
        self.iters_first = cfg['mapping']['iters_first']
        self.every_frame = cfg['mapping']['every_frame']
        self.color_refine = cfg['mapping']['color_refine']
        self.w_color_loss = cfg['mapping']['w_color_loss']
        self.w_geo_loss = cfg['mapping']['w_geo_loss']
        self.lambda_ssim_loss = cfg['mapping']['lambda_ssim_loss']
        self.keyframe_every = cfg['mapping']['keyframe_every']
        self.geo_iter_ratio = cfg['mapping']['geo_iter_ratio']
        self.mapping_window_size = cfg['mapping']['mapping_window_size']
        self.frustum_feature_selection = cfg['mapping']['frustum_feature_selection']
        self.keyframe_selection_method = cfg['mapping']['keyframe_selection_method']
        self.save_selected_keyframes_info = cfg['mapping']['save_selected_keyframes_info']
        self.frustum_edge = cfg['mapping']['frustum_edge']
        self.save_ckpts = cfg['mapping']['save_ckpts']
        self.crop_edge = 0 if cfg['cam']['crop_edge'] is None else cfg['cam']['crop_edge']
        self.min_iter_ratio = cfg['mapping']['min_iter_ratio']
        self.lazy_start = cfg['mapping']['lazy_start']

        if self.save_selected_keyframes_info:
            self.selected_keyframes = {}

        self.keyframe_dict = []
        self.keyframe_list = []

        self.logger = Logger(cfg, args, self)
        self.position_lr_init = 0.0001
        self.feature_lr = 0.0025
        self.opacity_lr = 0.05
        self.scaling_lr = 0.0001
        self.rotation_lr = 0.001

        self.fovx = focal2fov(self.fx, self.W)
        self.fovy = focal2fov(self.fy, self.H)
        self.zfar = 100.0
        self.znear = 0.01
        self.projection_matrix = getProjectionMatrix(znear=self.znear, zfar=self.zfar, fovX=self.fovx, fovY=self.fovy).transpose(0,1).cuda()

        self.gaussians_xyz = None
        self.gaussians_features_dc = None
        self.gaussians_features_rest = None
        self.gaussians_opacity = None
        self.gaussians_scaling = None
        self.gaussians_rotation = None
        self.gaussians_creation_frame_id = None # <--- [NEW]
        self.gaussians_ghost_count = None       # [NEW]


        self.scaling_activation = torch.exp
        self.scaling_inverse_activation = torch.log
        self.opacity_activation = torch.sigmoid
        self.rotation_activation = torch.nn.functional.normalize

        self.gaussians_xyz_grad = None
        self.gaussians_features_dc_grad = None
        self.gaussians_features_rest_grad = None
        self.gaussians_opacity_grad = None
        self.gaussians_scaling_grad = None
        self.gaussians_rotation_grad = None

        self.xyz_gradient_accum = torch.empty(0)
        self.denom = torch.empty(0)
        self.max_radii2D = None
        self.optimizer = None

        self.cam_lr = cfg['tracking']['lr']
        self.num_cam_iters = cfg['tracking']['iters']
        self.tracking_pixels = cfg['tracking']['pixels']
        self.separate_LR = cfg['tracking']['separate_LR']
        self.w_color_loss_tracking = cfg['tracking']['w_color_loss']
        self.w_geo_loss_tracking = cfg['tracking']['w_geo_loss']
        self.ignore_edge_W = cfg['tracking']['ignore_edge_W']
        self.ignore_edge_H = cfg['tracking']['ignore_edge_H']
        self.handle_dynamic = cfg['tracking']['handle_dynamic']
        self.use_color_in_tracking = cfg['tracking']['use_color_in_tracking']
        self.const_speed_assumption = cfg['tracking']['const_speed_assumption']
        self.sample_with_color_grad = cfg['tracking']['sample_with_color_grad']
        self.depth_limit = cfg['tracking']['depth_limit']
        self.use_opacity_mask_for_loss = cfg['tracking']['use_opacity_mask_for_loss']
        self.ignore_outlier_depth_loss = cfg['tracking']['ignore_outlier_depth_loss']
        self.opacity_thres = cfg['tracking']['opacity_thres']

        self.prev_mapping_idx = -1
        self.bg_color = torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")
        self.opacity_mask = None
        self.count_bound = 0
        self.inv_pose = None

        self.exposure_feat_all = ([] if self.encode_exposure else None)
        
        self.print_output_desc()

    def print_output_desc(self):
        print(f"â­ï¸ ğŸŒ™  mapping begin!!")

    def update_scale(self):
        self.H *= self.scale_factor
        self.W *= self.scale_factor
        self.H = int(self.H)
        self.W = int(self.W)
        self.fx *= self.scale_factor
        self.fy *= self.scale_factor
        self.cx *= self.scale_factor
        self.cy *= self.scale_factor

    def update_cam(self):
        if 'crop_size' in self.cfg['cam']:
            crop_size = self.cfg['cam']['crop_size']
            sx = crop_size[1] / self.W
            sy = crop_size[0] / self.H
            self.fx = sx*self.fx
            self.fy = sy*self.fy
            self.cx = sx*self.cx
            self.cy = sy*self.cy
            self.W = crop_size[1]
            self.H = crop_size[0]
        if self.cfg['cam']['crop_edge'] > 0:
            self.H -= self.cfg['cam']['crop_edge']*2
            self.W -= self.cfg['cam']['crop_edge']*2
            self.cx -= self.cfg['cam']['crop_edge']
            self.cy -= self.cfg['cam']['crop_edge']
            self.edge = self.cfg['cam']['crop_edge']

    def replace_tensor_to_optimizer(self, tensor, name):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups:
            if group["name"] == name:
                stored_state = self.optimizer.state.get(group['params'][0], None)
                stored_state["exp_avg"] = torch.zeros_like(tensor)
                stored_state["exp_avg_sq"] = torch.zeros_like(tensor)

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter(tensor.requires_grad_(True))
                self.optimizer.state[group['params'][0]] = stored_state

                optimizable_tensors[group["name"]] = group["params"][0]
        return optimizable_tensors

    def prune_neural_point(self, min_opacity):
        prune_mask = (self.opacity_activation(self.gaussians_opacity_grad) < min_opacity).squeeze()
        scale_mask1 = torch.max(self.scaling_activation(self.gaussians_scaling_grad), dim=-1)[0] > 0.4
        scale_mask2 = torch.max(self.scaling_activation(self.gaussians_scaling_grad), dim=-1)[0] > \
                      (torch.min(self.scaling_activation(self.gaussians_scaling_grad), dim=-1)[0] * 36)
        prune_mask = (prune_mask | scale_mask1 | scale_mask2).detach()

        self.prune_points(prune_mask)

        torch.cuda.empty_cache()

    def prune_points(self, mask):
        valid_points_mask = ~mask
        optimizable_tensors = self._prune_optimizer(valid_points_mask)

        self.gaussians_xyz_grad = optimizable_tensors["xyz"]
        self.gaussians_features_dc_grad = optimizable_tensors["f_dc"]
        self.gaussians_features_rest_grad = optimizable_tensors["f_rest"]
        self.gaussians_opacity_grad = optimizable_tensors["opacity"]
        self.gaussians_scaling_grad = optimizable_tensors["scaling"]
        self.gaussians_rotation_grad = optimizable_tensors["rotation"]
        self.max_radii2D = self.max_radii2D[valid_points_mask]
        # <--- [NEW] Sync pruning for creation_frame_id
        if self.gaussians_creation_frame_id is not None:
             self.gaussians_creation_frame_id = self.gaussians_creation_frame_id[valid_points_mask]
        if self.gaussians_ghost_count is not None:
             self.gaussians_ghost_count = self.gaussians_ghost_count[valid_points_mask]

    def _prune_optimizer(self, mask):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups[:6]:
            stored_state = self.optimizer.state.get(group['params'][0], None)
            if stored_state is not None:
                stored_state["exp_avg"] = stored_state["exp_avg"][mask]
                stored_state["exp_avg_sq"] = stored_state["exp_avg_sq"][mask]

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter((group["params"][0][mask].requires_grad_(True)))
                self.optimizer.state[group['params'][0]] = stored_state

                optimizable_tensors[group["name"]] = group["params"][0]
            else:
                group["params"][0] = nn.Parameter(group["params"][0][mask].requires_grad_(True))
                optimizable_tensors[group["name"]] = group["params"][0]
        return optimizable_tensors

    def get_mask_from_c2w(self, c2w, depth_np):
        """
        Frustum feature selection based on current camera pose and depth image.
        Args:
            c2w (tensor): camera pose of current frame.
            depth_np (numpy.array): depth image of current frame. for each (x,y)<->(width,height)

        Returns:
            mask (tensor): mask for selected optimizable feature.
        """
        H, W, fx, fy, cx, cy, = self.H, self.W, self.fx, self.fy, self.cx, self.cy
        points = np.array(self.gaussians.get_xyz().cpu()).reshape(-1, 3)

        c2w = c2w.cpu().numpy()
        w2c = np.linalg.inv(c2w)
        ones = np.ones_like(points[:, 0]).reshape(-1, 1)
        homo_vertices = np.concatenate(
            [points, ones], axis=1).reshape(-1, 4, 1)
        cam_cord_homo = w2c@homo_vertices
        cam_cord = cam_cord_homo[:, :3]
        K = np.array([[fx, .0, cx], [.0, fy, cy], [.0, .0, 1.0]]).reshape(3, 3)
        
        uv = K@cam_cord
        z = uv[:, -1:]+1e-5
        uv = uv[:, :2]/z
        uv = uv.astype(np.float32)

        remap_chunk = int(3e4)
        depths = []
        for i in range(0, uv.shape[0], remap_chunk):
            depths += [cv2.remap(depth_np,
                                 uv[i:i+remap_chunk, 0],
                                 uv[i:i+remap_chunk, 1],
                                 interpolation=cv2.INTER_LINEAR)[:, 0].reshape(-1, 1)]
        depths = np.concatenate(depths, axis=0)

        edge = self.frustum_edge
        mask = (uv[:, 0] < W-edge)*(uv[:, 0] > edge) * \
            (uv[:, 1] < H-edge)*(uv[:, 1] > edge)

        zero_mask = (depths == 0)
        depths[zero_mask] = np.max(depths)
        mask = mask & (0 <= z[:, :, 0]) & (z[:, :, 0] <= depths+0.5)
        mask = mask.reshape(-1)
        return np.where(mask)[0].tolist(), np.where(~mask)[0].tolist()

    def keyframe_selection_overlap(self, gt_color, gt_depth, c2w, keyframe_dict, k, N_samples=8, pixels=200):
        """
        Select overlapping keyframes to the current camera observation.

        Args:
            gt_color (tensor): ground truth color image of the current frame.
            gt_depth (tensor): ground truth depth image of the current frame.
            c2w (tensor): camera to world matrix (3*4 or 4*4 both fine).
            keyframe_dict (list): a list containing info for each keyframe.
            k (int): number of overlapping keyframes to select.
            N_samples (int, optional): number of samples/points per ray. Defaults to 16.
            pixels (int, optional): number of pixels to sparsely sample 
                from the image of the current camera. Defaults to 100.
        Returns:
            selected_keyframe_list (list): list of selected keyframe id.
        """
        device = self.device
        H, W, fx, fy, cx, cy = self.H, self.W, self.fx, self.fy, self.cx, self.cy

        rays_o, rays_d, gt_depth, gt_color = get_samples(
            0, H, 0, W, pixels,
            fx, fy, cx, cy, c2w, gt_depth, gt_color, self.device, depth_filter=True)

        gt_depth = gt_depth.reshape(-1, 1)
        gt_depth = gt_depth.repeat(1, N_samples)
        t_vals = torch.linspace(0., 1., steps=N_samples).to(device)
        near = gt_depth*0.8
        far = gt_depth+0.5
        z_vals = near * (1.-t_vals) + far * (t_vals)
        pts = rays_o[..., None, :] + \
            rays_d[..., None, :] * z_vals[..., :, None]
        vertices = pts.reshape(-1, 3).cpu().numpy()
        list_keyframe = []
        for keyframeid, keyframe in enumerate(keyframe_dict):
            c2w = keyframe['est_c2w'].cpu().numpy()
            w2c = np.linalg.inv(c2w)
            ones = np.ones_like(vertices[:, 0]).reshape(-1, 1)
            homo_vertices = np.concatenate(
                [vertices, ones], axis=1).reshape(-1, 4, 1)
            cam_cord_homo = w2c@homo_vertices
            cam_cord = cam_cord_homo[:, :3]
            K = np.array([[fx, .0, cx], [.0, fy, cy],
                         [.0, .0, 1.0]]).reshape(3, 3)
            
            uv = K@cam_cord
            z = uv[:, -1:]+1e-5
            uv = uv[:, :2]/z
            uv = uv.astype(np.float32)
            edge = 20
            mask = (uv[:, 0] < W-edge)*(uv[:, 0] > edge) * \
                (uv[:, 1] < H-edge)*(uv[:, 1] > edge)
            
            mask = mask & (z[:, :, 0] > 0)  
            mask = mask.reshape(-1)
            percent_inside = mask.sum()/uv.shape[0]
            list_keyframe.append(
                {'id': keyframeid, 'percent_inside': percent_inside})

        list_keyframe = sorted(
            list_keyframe, key=lambda i: i['percent_inside'], reverse=True)

        selected_keyframe_list = [dic['id']
                                  for dic in list_keyframe if dic['percent_inside'] > 0.00]
        selected_keyframe_list = list(np.random.permutation(
            np.array(selected_keyframe_list))[:k])
        return selected_keyframe_list

    def reset_opacity(self):
        get_opacity = self.opacity_activation(self.gaussians_opacity_grad)
        opacities_new = inverse_sigmoid(torch.min(get_opacity, torch.ones_like(get_opacity)*0.01))
        optimizable_tensors = self.replace_tensor_to_optimizer(opacities_new, "opacity")
        self.gaussians_opacity_grad = optimizable_tensors["opacity"]    
    
    def optimize_cur_map(self, num_joint_iters, idx, cur_gt_color, cur_gt_depth, gt_cur_c2w, cur_c2w, color_refine=False, seg_mask = None):
        print(f"    -> [DEBUG GS] optimize_cur_map(). idx={idx}")
        H, W, fx, fy, cx, cy = self.H, self.W, self.fx, self.fy, self.cx, self.cy
        init = True if idx == 0 else False
        
        mlp_exposure_para_list = []
        gt_depth_np = cur_gt_depth.cpu().numpy()
        gt_depth = cur_gt_depth
        gt_color = cur_gt_color

        # ==============================================================================================
        # 1. è‡ªé€‚åº”å¢ç‚¹ (Adaptive Densification)
        # ==============================================================================================
        if idx == 0:
            # [ä¿®æ”¹] å»æ‰å¹³æ–¹é¡¹ï¼Œæ”¹ä¸ºçº¿æ€§æˆ–æ’å®šï¼Œé˜²æ­¢è¿œæ™¯åˆå§‹åŒ–è¿‡å¤šå™ªç‚¹
            # è®¾å®šä¸€ä¸ªæ¸©å’Œçš„æ·±åº¦ç³»æ•°ï¼Œä¾‹å¦‚æœ€å¤š 1.5 å€
            depth_factor = torch.clamp(gt_depth.median() / 2.5, min=1.0, max=1.5).item()
            add_pts_num = int(self.pixels_adding * depth_factor * 4) 
        else:
            add_pts_num = max(self.pixels_adding // 5, 600)
        # ä¿®æ­£: Mask ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„ seg_mask (1=Static)
        refined_mask = seg_mask

        # éšæœºé‡‡æ · (Random Sampling)
        batch_rays_o, batch_rays_d, batch_gt_depth, batch_gt_color, i, j = get_samples(
            0, H, 0, W, add_pts_num,
            fx, fy, cx, cy, cur_c2w, gt_depth, gt_color, self.device, depth_filter=True, return_index=True, seg_mask = refined_mask)
        
        # --- Warp Logic ---
        keyframe_len = idx
        warp_window = 3
        warp_depth_img_batch = []
        warp_pose_batch = []
        warp_gt_pose_batch = []
        if keyframe_len >= warp_window:
            for frame_id in range(1, warp_window + 1):
                query_id = frame_id * (-1)
                tmp_gt_depth = self.video.depths_gt[idx + query_id]
                tmp_gt_c2w = self.video.poses_gt[idx + query_id] 
                tmp_est_c2w = self.estimate_c2w_list[idx + query_id].cuda() 

                warp_pose_batch += [tmp_est_c2w]
                warp_depth_img_batch += [tmp_gt_depth]
                tmp_gt_c2w = torch.from_numpy(pose_matrix_from_quaternion(tmp_gt_c2w.cpu())).cuda()
                warp_gt_pose_batch += [tmp_gt_c2w]
        else:
            for frame_id in range(1, keyframe_len + 1):
                query_id = frame_id * (-1)
                tmp_gt_depth = self.video.depths_gt[idx + query_id] 
                tmp_gt_c2w = self.video.poses_gt[idx + query_id]
                tmp_est_c2w = self.estimate_c2w_list[idx + query_id].cuda()
                warp_pose_batch += [tmp_est_c2w]
                warp_depth_img_batch += [tmp_gt_depth]
                tmp_gt_c2w = torch.from_numpy(pose_matrix_from_quaternion(tmp_gt_c2w.cpu())).cuda()
                warp_gt_pose_batch += [tmp_gt_c2w]
        
        if idx > 0:
            warp_depth_batch = torch.stack(warp_depth_img_batch, dim=0)
            warp_est_pose_batch = torch.stack(warp_pose_batch, dim=0)
            warp_gt_pose_batch = torch.stack(warp_gt_pose_batch, dim=0)
            intrinsic = np.array([self.fx, self.fy, self.cx, self.cy])
            batch_mask = depth_warp_pixel(cur_c2w, warp_est_pose_batch, cur_gt_depth.unsqueeze(-1), warp_depth_batch.unsqueeze(-1), cur_gt_depth,
                               intrinsic, H, W, batch_rays_o, batch_rays_d, batch_gt_depth, batch_gt_depth)
            batch_rays_o = batch_rays_o[batch_mask]
            batch_rays_d = batch_rays_d[batch_mask]
            batch_gt_depth = batch_gt_depth[batch_mask]
            batch_gt_color = batch_gt_color[batch_mask]
            i = i[batch_mask]
            j = j[batch_mask]
        
        # [Fix] Initialize frame_pts_add
        frame_pts_add = 0

        if not color_refine:
            _ = self.gaussians.add_neural_points(batch_rays_o, batch_rays_d, batch_gt_depth, batch_gt_color,
                                                 dynamic_radius=self.dynamic_r_add[
                                                     j, i] if self.use_dynamic_radius else None,
                                                 current_frame_id=idx.item())
            frame_pts_add += _  # Accumulate added points

            # ======================================================================================
            # 2. æ¢¯åº¦é‡‡æ · + é™æ€é®ç½©é—¨ç¦
            # ======================================================================================
            if self.pixels_based_on_color_grad > 0:
                # [ä¿®æ”¹] æ˜¾å¼ä¼ å…¥ seg_mask=refined_maskï¼Œè®©é‡‡æ ·åœ¨æºå¤´å°±é¿å¼€åŠ¨æ€ç‰©ä½“
                batch_rays_o, batch_rays_d, batch_gt_depth, batch_gt_color, i, j = get_samples_with_pixel_grad(
                    0, H, 0, W, self.pixels_based_on_color_grad,
                    H, W, fx, fy, cx, cy, cur_c2w, gt_depth, gt_color, self.device,
                    depth_filter=True, return_index=True, 
                    seg_mask=refined_mask) # <--- æ’å…¥è¿™ä¸€è¡Œå‚æ•°
                
                # è¾¹ç•Œå®‰å…¨æ£€æŸ¥ + Mask è¿‡æ»¤ (è¿™éƒ¨åˆ†å¯ä»¥ä¿ç•™ä½œä¸ºåŒé‡ä¿é™©ï¼Œæˆ–è€…åº”å¯¹ç´¢å¼•è¶Šç•Œ)
                if refined_mask is not None:
                    MH, MW = refined_mask.shape
                    valid_mask_indices = (i >= 0) & (i < MH) & (j >= 0) & (j < MW)
                    
                    is_static = torch.zeros_like(i, dtype=torch.bool)
                    
                    if valid_mask_indices.any():
                        valid_i = i[valid_mask_indices]
                        valid_j = j[valid_mask_indices]
                        is_static[valid_mask_indices] = refined_mask[valid_i, valid_j]

                    if is_static.sum() > 0:
                        _ = self.gaussians.add_neural_points(
                            batch_rays_o[is_static], batch_rays_d[is_static], 
                            batch_gt_depth[is_static], batch_gt_color[is_static], 
                            is_pts_grad=True, 
                            dynamic_radius=self.dynamic_r_add[j[is_static], i[is_static]] if self.use_dynamic_radius else None,
                            current_frame_id=idx.item()
                        )
                        frame_pts_add += _  # Accumulate
                else:
                    _ = self.gaussians.add_neural_points(
                        batch_rays_o, batch_rays_d, batch_gt_depth, batch_gt_color, 
                        is_pts_grad=True, 
                        dynamic_radius=self.dynamic_r_add[j, i] if self.use_dynamic_radius else None,
                        current_frame_id=idx.item()
                    )
                    frame_pts_add += _  # Accumulate

        # ================= [Module 1 Update]: ä¸­å€¼å‡ ä½•æ ¡éªŒ (Median Geometry Check) =================
        if True: 
            with torch.no_grad():
                refined_mask = seg_mask 

                # 1. å‡†å¤‡æ·±åº¦æ•°æ®
                depth_np = cur_gt_depth.cpu().numpy()
                
                # [æ ¸å¿ƒåˆ›æ–°]: ä½¿ç”¨ 3x3 ä¸­å€¼æ»¤æ³¢æ¥åˆ¤æ–­"é£ç‚¹"
                # é€»è¾‘ï¼šçœŸå®çš„å‡ ä½•è¾¹ç¼˜åœ¨ 3x3 é‚»åŸŸå†…é€šå¸¸æœ‰æ”¯æ’‘ï¼Œè€Œå™ªå£°(é£ç‚¹)æ˜¯å­¤ç«‹çš„
                depth_median = cv2.medianBlur(depth_np, 3)
                depth_diff = np.abs(depth_np - depth_median)
                
                # å®šä¹‰"é£ç‚¹æ©ç ": æ·±åº¦ä¸é‚»åŸŸä¸­å€¼å·®å¼‚è¶…è¿‡ 5cm çš„ï¼Œè§†ä¸ºä¸å¯ä¿¡å™ªå£°
                flying_pixel_mask = torch.from_numpy(depth_diff > 0.05).to(self.device)

                # 2. è®¡ç®—æ¸²æŸ“å¯†åº¦ (Density Map)
                # ... (ä¿æŒåŸæœ‰çš„ render å’Œ density_map è®¡ç®—é€»è¾‘ä¸å˜) ...
                render_pkg = render(
                    self.gaussians.get_xyz(), 
                    self.gaussians.get_features_dc(), 
                    self.gaussians.get_features_rest(),
                    self.opacity_activation(self.gaussians.get_opacity()), 
                    self.scaling_activation(self.gaussians.get_scaling()), 
                    self.rotation_activation(self.gaussians.get_rotation()),
                    self.gaussians.get_active_sh_degree(), 
                    self.gaussians.get_max_sh_degree(),
                    cur_c2w[:3, 3], 
                    torch.inverse(cur_c2w).transpose(0, 1), 
                    self.projection_matrix, 
                    self.fovx, self.fovy, self.H, self.W
                )
                viewspace_pts = render_pkg["viewspace_points"]
                visible_mask = render_pkg["visibility_filter"]
                valid_pts = viewspace_pts[visible_mask]
                pts_x = (valid_pts[:, 0] / 2.0 + 0.5) * self.W 
                pts_y = (valid_pts[:, 1] / 2.0 + 0.5) * self.H
                pts_x = pts_x.long().clamp(0, self.W - 1)
                pts_y = pts_y.long().clamp(0, self.H - 1)
                density_map = torch.zeros((self.H, self.W), device=self.device, dtype=torch.float32)
                flat_indices = pts_y * self.W + pts_x
                ones = torch.ones_like(flat_indices, dtype=torch.float32)
                density_map.view(-1).scatter_add_(0, flat_indices, ones)

                # 3. è®¡ç®—çº¹ç†æ¢¯åº¦ (ä¿ç•™ï¼Œç”¨äºä¼˜å…ˆåœ¨çº¹ç†å¤„åŠ ç‚¹)
                gray = 0.299 * cur_gt_color[:,:,0] + 0.587 * cur_gt_color[:,:,1] + 0.114 * cur_gt_color[:,:,2]
                sobel_x = torch.abs(gray[:, 1:] - gray[:, :-1])
                sobel_y = torch.abs(gray[1:, :] - gray[:-1, :])
                grad_mag = torch.zeros_like(gray)
                grad_mag[:, 1:] += sobel_x
                grad_mag[1:, :] += sobel_y
                
                # 4. ç»„åˆ Mask
                # åŸºç¡€æ¡ä»¶ï¼šé™æ€ + æ·±åº¦æœ‰æ•ˆ + [ä¸æ˜¯é£ç‚¹]
                # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†é¿å¼€è¾¹ç¼˜ï¼Œè€Œæ˜¯é¿å¼€"é£ç‚¹"ï¼
                mask_np = refined_mask.cpu().numpy().astype(np.uint8)
                kernel = np.ones((3, 3), np.uint8) 
                mask_eroded = cv2.erode(mask_np, kernel, iterations=1)
                safe_static_mask = torch.from_numpy(mask_eroded).bool().to(self.device)
                
                base_condition = safe_static_mask & (cur_gt_depth > 0.1) & (~flying_pixel_mask)
                
                # ç­–ç•¥ A: è¡¥æ´ (Density=0) -> å¿…é¡»è¡¥ï¼Œä¸ç®¡æœ‰æ²¡æœ‰çº¹ç†
                fill_hole_mask = base_condition & (density_map == 0)
                
                # ç­–ç•¥ B: å¢å¼ºçº¹ç† (Density<3 ä¸” æœ‰çº¹ç†) -> å…è®¸åœ¨è¾¹ç¼˜åŠ ç‚¹ï¼
                # åªè¦ä¸æ˜¯é£ç‚¹ï¼ŒçœŸå®çš„å›¾åƒè¾¹ç¼˜æˆ‘ä»¬ç°åœ¨å…è®¸åŠ ç‚¹äº†
                fill_texture_mask = base_condition & (density_map < 3) & (grad_mag > 0.05)
                
                target_mask = fill_hole_mask | fill_texture_mask
                
                num_fill = target_mask.sum().item()

                if num_fill > 0:
                    # [æ¢å¤é¢„ç®—] æ¢å¤åˆ° 2000ï¼Œä¿è¯ç‚¹æ•°å›å‡
                    fill_budget = 2000 
                    candidates = torch.nonzero(target_mask)
                    if candidates.shape[0] > fill_budget:
                        indices = torch.randperm(candidates.shape[0])[:fill_budget]
                        selected_coords = candidates[indices]
                    else:
                        selected_coords = candidates
                    
                    v_sel = selected_coords[:, 0]; u_sel = selected_coords[:, 1]
                    depth_new = cur_gt_depth[v_sel, u_sel]; color_new = cur_gt_color[v_sel, u_sel]
                    
                    x_new = (u_sel - cx) * depth_new / fx
                    y_new = (v_sel - cy) * depth_new / fy
                    z_new = depth_new
                    pts_c = torch.stack([x_new, y_new, z_new], dim=-1)
                    pts_w = (pts_c @ cur_c2w[:3, :3].T) + cur_c2w[:3, 3]
                    
                    cam_center = cur_c2w[:3, 3]
                    rays_d = pts_w - cam_center
                    rays_d = rays_d / (torch.norm(rays_d, dim=-1, keepdim=True) + 1e-7)
                    rays_o = cam_center.expand_as(rays_d)
                    
                    self.gaussians.add_neural_points(
                        rays_o, rays_d, depth_new, color_new,
                        current_frame_id = idx.item()
                    )
          
            

        # åŸæœ‰çš„ä¼˜åŒ–å¾ªç¯ (ä¸è¦åŠ¨å®ƒ)
        for i in range(num_joint_iters):
             # ... existing optimization code ...
            if self.pixels_based_on_render and idx > 0:
                with torch.no_grad():
                    camera_center = cur_c2w[:3, 3]
                    world_view_transform = torch.inverse(cur_c2w).transpose(0, 1)
                    render_pkg = render(self.gaussians.get_xyz(), self.gaussians.get_features_dc(), self.gaussians.get_features_rest(),
                                        self.opacity_activation(self.gaussians.get_opacity()), 
                                        self.scaling_activation(self.gaussians.get_scaling()),
                                        self.rotation_activation(self.gaussians.get_rotation()),
                                        self.gaussians.get_active_sh_degree(), self.gaussians.get_max_sh_degree(),
                                        camera_center, world_view_transform, self.projection_matrix, self.fovx, self.fovy, self.H, self.W)
                    
                    depth_render = render_pkg["depth"][0]
                    image_render = render_pkg["render"].permute(1, 2, 0)
                    opacity_render = render_pkg["acc"][0]
                    
                    batch_rays_o, batch_rays_d, batch_gt_depth, batch_gt_color, i, j = get_samples_point_add(
                        depth_render, image_render, opacity_render, 6000, self.add_pixel_depth_th, 0, H, 0, W,
                        H, W, fx, fy, cx, cy, cur_c2w, gt_depth, gt_color, self.device,
                        depth_filter=True, return_index=True, seg_mask = refined_mask)

                    # --- Render Warp Logic ---
                    keyframe_len = idx
                    warp_window = 3
                    warp_depth_img_batch = []
                    warp_pose_batch = []
                    warp_gt_pose_batch = []
                    if keyframe_len >= warp_window:
                        for frame_id in range(1, warp_window + 1):
                            query_id = frame_id * (-1)
                            tmp_gt_depth = self.video.depths_gt[idx + query_id]
                            tmp_gt_c2w = self.video.poses_gt[idx + query_id]
                            tmp_est_c2w = self.estimate_c2w_list[idx + query_id].cuda()
                            warp_pose_batch += [tmp_est_c2w]
                            warp_depth_img_batch += [tmp_gt_depth]
                            tmp_gt_c2w = torch.from_numpy(pose_matrix_from_quaternion(tmp_gt_c2w.cpu())).cuda()
                            warp_gt_pose_batch += [tmp_gt_c2w]
                    else:
                        for frame_id in range(1, keyframe_len + 1):
                            query_id = frame_id * (-1)
                            tmp_gt_depth = self.video.depths_gt[idx + query_id]                            
                            tmp_gt_c2w = self.video.poses_gt[idx + query_id]
                            tmp_est_c2w = self.estimate_c2w_list[idx + query_id].cuda()
                            warp_pose_batch += [tmp_est_c2w]
                            warp_depth_img_batch += [tmp_gt_depth]
                            tmp_gt_c2w = torch.from_numpy(pose_matrix_from_quaternion(tmp_gt_c2w.cpu())).cuda()
                            warp_gt_pose_batch += [tmp_gt_c2w]
                    if idx > 0:
                        warp_depth_batch = torch.stack(warp_depth_img_batch, dim=0)
                        warp_est_pose_batch = torch.stack(warp_pose_batch, dim=0)
                        warp_gt_pose_batch = torch.stack(warp_gt_pose_batch, dim=0)
                        intrinsic = np.array([self.fx, self.fy, self.cx, self.cy])
                        batch_mask = depth_warp_pixel(cur_c2w, warp_est_pose_batch, cur_gt_depth.unsqueeze(-1),
                                                      warp_depth_batch.unsqueeze(-1), cur_gt_depth,
                                                      intrinsic, H, W, batch_rays_o, batch_rays_d, batch_gt_depth,
                                                      batch_gt_depth)
                        batch_rays_o = batch_rays_o[batch_mask]
                        batch_rays_d = batch_rays_d[batch_mask]
                        batch_gt_depth = batch_gt_depth[batch_mask]
                        batch_gt_color = batch_gt_color[batch_mask]
                        i = i[batch_mask]
                        j = j[batch_mask]
                    
                    _ = self.gaussians.add_neural_points(batch_rays_o, batch_rays_d, batch_gt_depth, batch_gt_color,
                                                         dynamic_radius=self.dynamic_r_add[j, i] if self.use_dynamic_radius else None,
                                                         current_frame_id=idx.item())
                    frame_pts_add += _  # Accumulate
        
        # ==========================================================
        # 3. å‡†å¤‡æ•°æ® (Seen/Unseen)
        # ==============================================================================================
        self.gaussians_xyz = self.gaussians.get_xyz()
        self.gaussians_features_dc = self.gaussians.get_features_dc()
        self.gaussians_features_rest = self.gaussians.get_features_rest()
        self.gaussians_opacity = self.gaussians.get_opacity()
        self.gaussians_scaling = self.gaussians.get_scaling()
        self.gaussians_rotation = self.gaussians.get_rotation()
        self.gaussians_creation_frame_id = self.gaussians.get_creation_frame_id() 
        self.gaussians_ghost_count = self.gaussians.get_ghost_count()

        masked_c_grad = {}
        mask_c2w = cur_c2w
        indices, indices_unseen = self.get_mask_from_c2w(mask_c2w, gt_depth_np)

        gaussians_xyz_unfrustum = self.gaussians_xyz[indices_unseen].detach().clone()
        gaussians_features_dc_unfrustum = self.gaussians_features_dc[indices_unseen].detach().clone()
        gaussians_features_rest_unfrustum = self.gaussians_features_rest[indices_unseen].detach().clone()
        gaussians_opacity_unfrustum = self.gaussians_opacity[indices_unseen].detach().clone()
        gaussians_scaling_unfrustum = self.gaussians_scaling[indices_unseen].detach().clone()
        gaussians_rotation_unfrustum = self.gaussians_rotation[indices_unseen].detach().clone()
        gaussians_creation_id_unfrustum = self.gaussians_creation_frame_id[indices_unseen].detach().clone()
        gaussians_ghost_count_unfrustum = self.gaussians_ghost_count[indices_unseen].detach().clone()

        self.gaussians_xyz_grad = self.gaussians_xyz[indices].detach().clone().requires_grad_(True)
        self.gaussians_features_dc_grad = self.gaussians_features_dc[indices].detach().clone().requires_grad_(True)
        self.gaussians_features_rest_grad = self.gaussians_features_rest[indices].detach().clone().requires_grad_(True)
        self.gaussians_opacity_grad = self.gaussians_opacity[indices].detach().clone().requires_grad_(True)
        self.gaussians_scaling_grad = self.gaussians_scaling[indices].detach().clone().requires_grad_(True)
        self.gaussians_rotation_grad = self.gaussians_rotation[indices].detach().clone().requires_grad_(True)
        self.gaussians_creation_frame_id = self.gaussians_creation_frame_id[indices].detach().clone()
        self.gaussians_ghost_count = self.gaussians_ghost_count[indices].detach().clone()
        
        masked_c_grad['indices'] = indices

        if self.encode_exposure:
            mlp_exposure_para_list += list(self.mlp_exposure.parameters())
        optim_para_list = [
            {'params': [self.gaussians_xyz_grad], 'lr': self.position_lr_init, "name": "xyz"},
            {'params': [self.gaussians_features_dc_grad], 'lr': self.feature_lr, "name": "f_dc"},
            {'params': [self.gaussians_features_rest_grad], 'lr': self.feature_lr / 20.0, "name": "f_rest"},
            {'params': [self.gaussians_opacity_grad], 'lr': self.opacity_lr, "name": "opacity"},
            {'params': [self.gaussians_scaling_grad], 'lr': self.scaling_lr, "name": "scaling"},
            {'params': [self.gaussians_rotation_grad], 'lr': self.rotation_lr, "name": "rotation"}
        ]
        if self.encode_exposure:
            optim_para_list.append({'params': self.exposure_feat, 'lr': 0.001, "name": "expos_feat"})
            optim_para_list.append({'params': mlp_exposure_para_list, 'lr': 0.005, "name": "mlp_expos_para"})
        self.optimizer = torch.optim.Adam(optim_para_list)

        if idx > 0 and not color_refine:
            num_joint_iters = 60
            num_joint_iters = np.clip(int(num_joint_iters * frame_pts_add / 5000), int(
                self.min_iter_ratio * num_joint_iters), 2 * num_joint_iters)
        
        num_joint_iters = max(num_joint_iters, 60)
        
        actual_joint_iters = 0

        for joint_iter in range(num_joint_iters):
            if joint_iter <= (self.geo_iter_first if init else int(num_joint_iters * self.geo_iter_ratio)):
                self.stage = 'geometry'
            else:
                self.stage = 'color'

            self.optimizer.zero_grad()

            exposure_feat_list = []

            gt_depth = cur_gt_depth
            gt_color = cur_gt_color
            c2w = cur_c2w

            if self.encode_exposure:
                exposure_feat_list.append(self.exposure_feat)

            camera_center = c2w[:3, 3]
            world_view_transform = torch.inverse(c2w).transpose(0, 1)
            gaussians_opacity_grad_activation = self.opacity_activation(self.gaussians_opacity_grad)
            gaussians_scaling_grad_activation = self.scaling_activation(self.gaussians_scaling_grad)
            gaussians_rotation_grad_activation = self.rotation_activation(self.gaussians_rotation_grad)

            render_pkg = render(self.gaussians_xyz_grad, self.gaussians_features_dc_grad,
                                self.gaussians_features_rest_grad, \
                                gaussians_opacity_grad_activation, gaussians_scaling_grad_activation,
                                gaussians_rotation_grad_activation, \
                                self.gaussians.get_active_sh_degree(), self.gaussians.get_max_sh_degree(),
                                camera_center, world_view_transform, self.projection_matrix, \
                                self.fovx, self.fovy, self.H, self.W)

            image, viewspace_point_tensor, visibility_filter, radii = render_pkg["render"], render_pkg[
                "viewspace_points"], render_pkg["visibility_filter"], render_pkg["radii"]
            depth = render_pkg["depth"][0]
            image = image.permute(1, 2, 0)

            if self.encode_exposure:
                image = image.reshape(-1, 3)
                affine_tensor = self.mlp_exposure(exposure_feat_list[-1])
                rot, trans = affine_tensor[:9].reshape(3, 3), affine_tensor[-3:]
                image_slice = image.clone()
                image_slice = torch.matmul(image_slice, rot) + trans
                image = image_slice
                image = torch.sigmoid(image)
                image = image.reshape(self.H, self.W, 3)
            
            if joint_iter % 30 == 0:
                self.gaussians.oneupSHdegree()

            mask = (gt_depth > 0.0) & (gt_depth < 8.) & refined_mask
            mask = mask & (~torch.isnan(gt_depth))
            depths_wmask = depth[mask]
            gt_depths_wmask = gt_depth[mask]

            # [ä¿®æ”¹] å¼•å…¥è·ç¦»æƒé‡è¡°å‡ (Depth Confidence Decay)
            # è¿œäº 4m çš„ç‚¹ï¼Œå‡ ä½•æƒé‡é™ä¸º 0.1ï¼›è¿‘å¤„ä¿æŒ 1.0
            dist_weights = torch.ones_like(gt_depths_wmask)
            dist_weights[gt_depths_wmask > 4.0] = 0.1 
            
            # åº”ç”¨æƒé‡
            geo_loss = (torch.abs(gt_depths_wmask - depths_wmask) * dist_weights).sum()
            loss = geo_loss.clone() * self.w_geo_loss
            color_mask = mask
            color_loss = torch.abs(gt_color[color_mask] - image[color_mask]).sum()

            ssim_loss = (1.0 - ssim(image.permute(2, 0, 1), gt_color.permute(2, 0, 1).float()))
            weighted_ssim_loss = self.lambda_ssim_loss * ssim_loss
            loss += weighted_ssim_loss
            weighted_color_loss = self.w_color_loss * color_loss * (1 - self.lambda_ssim_loss)
            loss += weighted_color_loss

            loss.backward(retain_graph=False)
            
            self.max_radii2D = radii
            with torch.no_grad():
                #if joint_iter == num_joint_iters - 1:
                     #self.prune_dynamic_ghosting(cur_c2w, cur_gt_depth, cur_gt_color, refined_mask, idx.item(), age_thres=0)

                self.optimizer.step()
                self.optimizer.zero_grad()
                actual_joint_iters += 1

            total_gs_point_num_prune = self.gaussians_xyz.shape[0]
            if not self.wandb:
                if joint_iter % 100 == 0:
                    if self.stage == 'geometry':
                        print('iter: ', joint_iter, ', geo_loss: ', f'{geo_loss.item():0.6f}')
                    else:
                        print('iter: ', joint_iter, ', geo_loss: ', f'{geo_loss.item():0.6f}', ', color_loss: ', f'{color_loss.item():0.6f}')

            if joint_iter == num_joint_iters - 1:
                print('idx: ', idx.item(), ', geo_loss_pixel: ',
                      f'{(geo_loss.item() / mask.sum().item()):0.6f}',
                      ', color_loss_pixel: ', f'{(color_loss.item() / color_mask.sum().item()):0.4f}')
                if self.wandb:
                    wandb.log({'idx': int(idx.item()),
                                'geo_loss_pixel': float(f'{(geo_loss.item() / mask.sum().item()):0.6f}'),
                                'color_loss_pixel': float(f'{(color_loss.item() / color_mask.sum().item()):0.6f}'),
                                'pts_total': total_gs_point_num_prune})
                    wandb.log({'idx_map': int(idx.item()),
                               'num_joint_iters': num_joint_iters})

        # ================= [Module 2 Update]: å…»è€é‡‘å¤è¾Ÿ (Pension Restored) =================
        if True: 
            with torch.no_grad():
                opacity = self.opacity_activation(self.gaussians_opacity_grad)
                scaling = self.scaling_activation(self.gaussians_scaling_grad)
                creation_ids = self.gaussians_creation_frame_id
                
                point_age = idx.item() - creation_ids.squeeze()
                
                # å®šä¹‰çŠ¶æ€
                is_old = point_age >= 5         # æ´»è¿‡5ä¸ªå…³é”®å¸§ç®—è€ç‚¹
                is_giant = scaling.max(dim=1).values > 0.8
                
                # [å…³é”®ä¿®æ­£]ï¼šåªæ€"å¹´è½»ä¸”é€æ˜"çš„ï¼Œæ”¾è¿‡"å¹´è€ä¸”é€æ˜"çš„
                # ä¹‹å‰çš„ opacity_thresh = 0.1 ä¸”æ— è§†å¹´é¾„ï¼Œå¯¼è‡´è€ç‚¹ä¹Ÿè¢«æ€å…‰äº†
                
                # 1. åƒåœ¾æ¸…ç†ï¼šå¹´è½»(Age<5) ä¸” é€æ˜åº¦ä½(<0.1) -> æ€
                mask_noise_young = (opacity.squeeze() < 0.10) & (~is_old)
                
                # 2. æ·±åº¦æ¸…ç†ï¼šæåº¦é€æ˜(<0.01) -> æ— è®ºå¤šè€éƒ½æ€ (é˜²æ­¢æ˜¾å­˜æº¢å‡º)
                mask_noise_dead = opacity.squeeze() < 0.01

                # 3. å·¨äººæ€æ‰‹ï¼šå¹´è½»çš„å¤§çƒ -> æ€ (ä¿ç•™è€çš„èƒŒæ™¯å¢™)
                mask_giant = is_giant & (~is_old)

                kill_mask = mask_noise_young | mask_noise_dead | mask_giant
                
                if kill_mask.sum() > 0:
                    self.prune_points(kill_mask)
                    print(f"[Module 2] Pruned {kill_mask.sum()} pts (Pension Active)")

        # ======================================================================================
        
        
        self.gaussians_xyz = torch.cat((self.gaussians_xyz_grad.detach().clone(), gaussians_xyz_unfrustum.detach().clone()), 0)
        self.gaussians_features_dc = torch.cat((self.gaussians_features_dc_grad.detach().clone(), gaussians_features_dc_unfrustum.detach().clone()), 0)
        self.gaussians_features_rest = torch.cat((self.gaussians_features_rest_grad.detach().clone(), gaussians_features_rest_unfrustum.detach().clone()), 0)
        self.gaussians_opacity = torch.cat((self.gaussians_opacity_grad.detach().clone(), gaussians_opacity_unfrustum.detach().clone()), 0)
        self.gaussians_scaling = torch.cat((self.gaussians_scaling_grad.detach().clone(), gaussians_scaling_unfrustum.detach().clone()), 0)
        self.gaussians_rotation = torch.cat((self.gaussians_rotation_grad.detach().clone(), gaussians_rotation_unfrustum.detach().clone()), 0)
        
        self.gaussians_creation_frame_id = torch.cat((self.gaussians_creation_frame_id, gaussians_creation_id_unfrustum), 0)
        self.gaussians_ghost_count = torch.cat((self.gaussians_ghost_count, gaussians_ghost_count_unfrustum), 0)

        self.gaussians.update_xyz(self.gaussians_xyz.detach().clone())
        self.gaussians.update_features_dc(self.gaussians_features_dc.detach().clone())
        self.gaussians.update_features_rest(self.gaussians_features_rest.detach().clone())
        self.gaussians.update_scaling(self.gaussians_scaling.detach().clone())
        self.gaussians.update_rotation(self.gaussians_rotation.detach().clone())
        self.gaussians.update_opacity(self.gaussians_opacity.detach().clone())
        
        self.gaussians.update_creation_frame_id(self.gaussians_creation_frame_id.detach().clone())
        self.gaussians.update_ghost_count(self.gaussians_ghost_count.detach().clone())
        
        print('Current Map has been updated (Pruning Active)')
        
        # ================= [å¯è§†åŒ– V2ï¼šæ˜¾å½¢æ¨¡å¼] =================
        if True:
            with torch.no_grad():
                try:
                    xyz = self.gaussians.get_xyz()
                    # [æ–°å¢] è·å– Scaleï¼Œç”¨äºç”»åœˆ
                    scales = self.gaussians.get_scaling() 
                    # å– Scale çš„æœ€å¤§åˆ†é‡ä½œä¸ºå¯è§†åŒ–åŠå¾„çš„å‚è€ƒ (ç®€å•è¿‘ä¼¼)
                    max_scales = scales.max(dim=1).values 

                    w2c = torch.inverse(cur_c2w)
                    R = w2c[:3, :3]; T = w2c[:3, 3]
                    pts_cam = (xyz @ R.T) + T
                    x, y, z = pts_cam[:, 0], pts_cam[:, 1], pts_cam[:, 2]
                    
                    # [ä¿®æ­£ 1] æ”¾å®½æ·±åº¦é™åˆ¶åˆ° 100ç±³ (å’Œæ¸²æŸ“å™¨ä¸€è‡´)
                    valid_mask = (z > 0.01) & (z < 100.0) 
                    
                    # æŠ•å½±
                    u = (x[valid_mask] / z[valid_mask] * self.fx + self.cx).long()
                    v = (y[valid_mask] / z[valid_mask] * self.fy + self.cy).long()
                    s = max_scales[valid_mask] # è·å–å¯¹åº”ç‚¹çš„ scale
                    
                    H, W = self.H, self.W
                    valid_uv = (u >= 0) & (u < W) & (v >= 0) & (v < H)
                    
                    u_final = u[valid_uv].cpu().numpy()
                    v_final = v[valid_uv].cpu().numpy()
                    s_final = s[valid_uv].cpu().numpy()
                    
                    # ç»˜åˆ¶
                    viz_img = np.zeros((H, W, 3), dtype=np.uint8)
                    
                    # [ä¿®æ­£ 2] ç»˜åˆ¶å®å¿ƒåœ†ï¼ŒåŠå¾„ä¸ Scale æˆæ­£æ¯”
                    # è¿™é‡Œçš„åŠå¾„è®¡ç®—æ˜¯ç²—ç•¥çš„ï¼Œä¸ºäº†è§†è§‰æ˜¾è‘—ï¼Œæˆ‘ä»¬æŠŠ scale æ”¾å¤§ä¸€äº›
                    for i in range(len(u_final)):
                        # ç®€å•çš„é€è§†æŠ•å½±åŠå¾„ä¼°è®¡: radius = scale * fx / z
                        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œç›´æ¥ç»™ä¸€ä¸ªåŸºç¡€å¤§å° + scale å› å­
                        radius = max(1, int(s_final[i] * 50)) 
                        # å¦‚æœæ˜¯å·¨å¤§çš„ç‚¹ï¼Œç”»æˆçº¢è‰²ä»¥ç¤ºè­¦
                        color = (0, 0, 255) if radius > 5 else (255, 255, 255)
                        cv2.circle(viz_img, (u_final[i], v_final[i]), radius, color, -1)
                    
                    save_dir = os.path.join(self.output, 'debug_distribution_fixed')
                    os.makedirs(save_dir, exist_ok=True)
                    cv2.imwrite(os.path.join(save_dir, f'{idx:05d}_points_reveal.png'), viz_img)
                    
                    print(f"[Viz Check] Frame {idx}: Saved revealed point cloud (Z<100m).")
                    
                except Exception as e:
                    print(f"Viz Error: {e}")

        
        if self.encode_exposure and idx == (self.n_img - 1):
            self.exposure_feat_all.append(self.exposure_feat.detach().cpu())
        return None

    def prune_dynamic_ghosting(self, cur_c2w, gt_depth, gt_color, seg_mask, current_frame_idx, age_thres=0):
        """
        ä¿®æ­£ç‰ˆ V6ï¼šåŠ å…¥ Opacity Boosting Trick
        ç›®çš„ï¼šå¼ºåˆ¶è®©åŠé€æ˜çš„é¬¼å½±å˜"å®"ï¼Œä»¥ä¾¿æ·±åº¦æ£€æµ‹èƒ½æ•æ‰åˆ°å®ƒä»¬ã€‚
        """
        with torch.no_grad():
            # 1. æ¸²æŸ“ (Trick: Opacity Boosting)
            camera_center = cur_c2w[:3, 3]
            world_view_transform = torch.inverse(cur_c2w).transpose(0, 1)
            
            # [å…³é”® Trick] æ”¾å¤§é€æ˜åº¦
            opacity = self.opacity_activation(self.gaussians_opacity_grad)
            boosted_opacity = torch.clamp(opacity * 10.0, max=0.999)
            
            render_pkg = render(self.gaussians_xyz_grad, self.gaussians_features_dc_grad, self.gaussians_features_rest_grad,
                                boosted_opacity, # ä½¿ç”¨ Boost åçš„é€æ˜åº¦
                                self.scaling_activation(self.gaussians_scaling_grad),
                                self.rotation_activation(self.gaussians_rotation_grad),
                                self.gaussians.get_active_sh_degree(), self.gaussians.get_max_sh_degree(),
                                camera_center, world_view_transform, self.projection_matrix, 
                                self.fovx, self.fovy, self.H, self.W)
            
            depth_render = render_pkg["depth"][0]
            
            # ==============================================================================
            # æ·±åº¦è¾¹ç¼˜æ£€æµ‹ (Edge Protection)
            # ==============================================================================
            g_depth = gt_depth
            dy = torch.abs(g_depth[1:, :] - g_depth[:-1, :])
            dx = torch.abs(g_depth[:, 1:] - g_depth[:, :-1])
            dy = torch.nn.functional.pad(dy, (0, 0, 0, 1))
            dx = torch.nn.functional.pad(dx, (0, 1, 0, 0))
            edge_mask = (dy > 0.1) | (dx > 0.1)

            # ==============================================================================
            # 2. åˆ¤å®šé€»è¾‘
            # ==============================================================================
            depth_diff = gt_depth - depth_render
            
            # ç‰©ç†é®æŒ¡: æ¸²æŸ“æ¯”çœŸå€¼è¿‘ 5cm
            is_blocking = (depth_diff > 0.05) & (gt_depth > 0)
            
            # é¬¼å½±åˆ¤å®š: ç‰©ç†æŒ¡è·¯ & è¯­ä¹‰èƒŒæ™¯ & éè¾¹ç¼˜
            pixel_guilty_mask = is_blocking & seg_mask & (gt_depth > 0) & (~edge_mask)
            
            # æ´—ç™½åˆ¤å®š
            pixel_innocent_mask = (torch.abs(depth_diff) < 0.02) & (gt_depth > 0)

            if pixel_guilty_mask.sum() == 0:
                return

            # 3. ç­›é€‰å«Œç–‘ç‚¹
            if self.gaussians_creation_frame_id is None: return
            creation_ids = self.gaussians_creation_frame_id.squeeze()
            ghost_counts = self.gaussians_ghost_count.squeeze()
            
            suspect_xyz = self.gaussians_xyz_grad

            # 4. æŠ•å½±å…³è”
            w2c = torch.inverse(cur_c2w)
            R = w2c[:3, :3]; T = w2c[:3, 3]
            pts_cam = (suspect_xyz @ R.T) + T
            x, y, z = pts_cam[:, 0], pts_cam[:, 1], pts_cam[:, 2]
            
            u = (x/z * self.fx + self.cx).long()
            v = (y/z * self.fy + self.cy).long()
            valid_uv = (z > 0.01) & (u >= 0) & (u < self.W) & (v >= 0) & (v < self.H)
            
            if valid_uv.any():
                valid_indices_global = torch.where(valid_uv)[0]
                u_valid, v_valid = u[valid_uv], v[valid_uv]
                z_valid = z[valid_uv]
                
                # Z-Check
                depth_at_pixel = depth_render[v_valid, u_valid]
                is_contributor = (z_valid <= (depth_at_pixel + 0.05))
                
                is_pixel_guilty = pixel_guilty_mask[v_valid, u_valid]
                is_pixel_innocent = pixel_innocent_mask[v_valid, u_valid]
                

                # =================== Step 5: æ›´æ–°è®¡æ•° (ä¿®æ­£ç‰ˆ) ===================
                
                # 1. å®šä¹‰å…¨å›¾çš„é‡ç½ª mask (è¿™æ˜¯ HxW çš„å›¾)
                is_heavy_crime_map = (depth_diff > 0.20)
                
                # [å…³é”®ä¿®æ­£]ï¼šå¿…é¡»ä»å›¾ä¸­å–å‡ºå¯¹åº”åƒç´ ç‚¹çš„å€¼ï¼Œä½¿å…¶å˜æˆ (N,) çš„å‘é‡
                # è¿™æ ·å®ƒæ‰èƒ½å’Œ is_pixel_guilty (N,) è¿›è¡Œè¿ç®—
                is_heavy_crime = is_heavy_crime_map[v_valid, u_valid]
                
                # 2. é‡ç½ªç‚¹ (Heavy Guilty)
                mask_heavy = is_pixel_guilty & is_heavy_crime
                heavy_points = is_contributor & mask_heavy
                idx_heavy = valid_indices_global[heavy_points]
                
                if len(idx_heavy) > 0:
                    ghost_counts[idx_heavy] += 10 # ä¸¥å‰æƒ©ç½š
                
                # 3. è½»ç½ªç‚¹ (Light Guilty)
                # åŒæ ·ï¼Œè¿™é‡Œçš„è¿ç®—ç°åœ¨éƒ½æ˜¯ (N,) å¯¹ (N,) äº†ï¼Œæ˜¯åˆæ³•çš„
                mask_light = is_pixel_guilty & (~is_heavy_crime)
                light_points = is_contributor & mask_light
                idx_light = valid_indices_global[light_points]
                
                if len(idx_light) > 0:
                    ghost_counts[idx_light] += 1 # æ ‡å‡†æƒ©ç½š

                # 4. å¥–åŠ±é€»è¾‘ (ä¿æŒä¸å˜)
                innocent_points = is_contributor & is_pixel_innocent
                idx_to_reward = valid_indices_global[innocent_points]
                if len(idx_to_reward) > 0:
                    ghost_counts[idx_to_reward] = torch.clamp(ghost_counts[idx_to_reward] - 1, min=0)

                # 6. åˆ†çº§å¤„å†³ (Hierarchical Execution)
                point_ages = current_frame_idx - creation_ids
                prune_thresholds = torch.ones_like(ghost_counts) * 3 
                prune_thresholds[point_ages > 3] = 10
                prune_thresholds[point_ages > 10] = 50
                
                death_mask = (ghost_counts > prune_thresholds)
                
                # Debug Visualization
                if death_mask.sum() > 0:
                    try:
                        prune_xyz = self.gaussians_xyz_grad[death_mask]
                        pts_cam_p = (prune_xyz @ R.T) + T
                        x_p, y_p, z_p = pts_cam_p[:, 0], pts_cam_p[:, 1], pts_cam_p[:, 2]
                        valid_z_p = z_p > 0.01
                        
                        u_p = (x_p[valid_z_p] / z_p[valid_z_p] * self.fx + self.cx).long()
                        v_p = (y_p[valid_z_p] / z_p[valid_z_p] * self.fy + self.cy).long()
                        
                        valid_draw = (u_p >= 0) & (u_p < self.W) & (v_p >= 0) & (v_p < self.H)
                        u_draw = u_p[valid_draw].cpu().numpy()
                        v_draw = v_p[valid_draw].cpu().numpy()
                        
                        viz_img = (gt_color.cpu().numpy() * 255).astype(np.uint8).copy()
                        viz_img = cv2.cvtColor(viz_img, cv2.COLOR_RGB2BGR)
                        viz_img[v_draw, u_draw] = [0, 0, 255] 
                        
                        save_name = f"{current_frame_idx:05d}_pruned_{death_mask.sum().item()}.png"
                        save_path = os.path.join(self.debug_prune_dir, save_name)
                        cv2.imwrite(save_path, viz_img)
                    except Exception:
                        pass

                if death_mask.sum() > 0:
                    self.prune_points(death_mask)
    

    def optimize_map(self, num_joint_iters, idx, cur_gt_color, cur_gt_depth, gt_cur_c2w,
                     keyframe_dict, keyframe_list, cur_c2w, color_refine=False, cur_seg_mask = None):
        """
        Mapping iterations. Sample pixels from selected keyframes,
        then optimize scene representation and camera poses(if local BA enables).

        Args:
            num_joint_iters (int): number of mapping iterations.
            idx (int): the index of current frame
            cur_gt_color (tensor): gt_color image of the current camera.
            cur_gt_depth (tensor): gt_depth image of the current camera.
            gt_cur_c2w (tensor): groundtruth camera to world matrix corresponding to current frame.
            keyframe_dict (list): list of keyframes info dictionary.
            keyframe_list (list): list of keyframe index.
            cur_c2w (tensor): the estimated camera to world matrix of current frame. 
            color_refine (bool): whether to do color refinement (optimize color features with fixed color decoder).

        Returns:
            cur_c2w/None (tensor/None): return the updated cur_c2w, return None if no BA
        """
        print(f"    -> [DEBUG GS] optimize_map(). idx={idx}")
        H, W, fx, fy, cx, cy = self.H, self.W, self.fx, self.fy, self.cx, self.cy
        cfg = self.cfg
        device = self.device
        init = True if idx == 0 else False
        bottom = torch.tensor([0, 0, 0, 1.0], device=self.device).reshape(1, 4)

        if len(keyframe_dict) == 0:
            optimize_frame = []
        else:
            if self.keyframe_selection_method == 'global':
                num = self.mapping_window_size-2
                optimize_frame = random_select(len(self.keyframe_dict)-1, num)
            elif self.keyframe_selection_method == 'overlap':
                num = self.mapping_window_size-2
                optimize_frame = self.keyframe_selection_overlap(
                    cur_gt_color, cur_gt_depth, cur_c2w, keyframe_dict[:-1], num)

        oldest_frame = None
        if len(keyframe_list) > 0:
            optimize_frame = optimize_frame + [len(keyframe_list)-1]
            oldest_frame = min(optimize_frame)
        optimize_frame += [-1]

        opt_frames_camera_tensor_list = []
        opt_frames_gt_depth_list = []

        if self.save_selected_keyframes_info:
            keyframes_info = []
            for id, frame in enumerate(optimize_frame):
                if frame != -1:
                    frame_idx = keyframe_list[frame]
                    tmp_gt_c2w = keyframe_dict[frame]['gt_c2w']
                    tmp_est_c2w = keyframe_dict[frame]['est_c2w']
                    tmp_gt_depth = keyframe_dict[frame]['depth']
                else:
                    frame_idx = idx
                    tmp_gt_c2w = gt_cur_c2w
                    tmp_est_c2w = cur_c2w
                    tmp_gt_depth = cur_gt_depth
                keyframes_info.append(
                    {'idx': frame_idx, 'gt_c2w': tmp_gt_c2w, 'est_c2w': tmp_est_c2w})
                opt_frames_camera_tensor_list.append(tmp_est_c2w)
                opt_frames_gt_depth_list.append(tmp_gt_depth.cpu().numpy())
            self.selected_keyframes[idx] = keyframes_info

        mlp_exposure_para_list = []
        self.gaussians_xyz = self.gaussians.get_xyz()
        self.gaussians_features_dc = self.gaussians.get_features_dc()
        self.gaussians_features_rest = self.gaussians.get_features_rest()
        self.gaussians_opacity = self.gaussians.get_opacity()
        self.gaussians_scaling = self.gaussians.get_scaling()
        self.gaussians_rotation = self.gaussians.get_rotation()
        self.gaussians_creation_frame_id = self.gaussians.get_creation_frame_id() # <--- [NEW]
        self.gaussians_ghost_count = self.gaussians.get_ghost_count()  #[NEW]
        indices = None
        indices_unseen = None
        if self.frustum_feature_selection:  # required if not color_refine
            masked_c_grad = {}
            for i in range(len(opt_frames_camera_tensor_list)):
                mask_c2w = opt_frames_camera_tensor_list[i]
                mask_depth_np = opt_frames_gt_depth_list[i]
                indices_i, indices_us_i = self.get_mask_from_c2w(mask_c2w, mask_depth_np)
                indices = list(set(indices) | (set(indices_i))) if indices is not None else indices_i
                indices_unseen = list(set(indices_unseen) & (set(indices_us_i))) if indices_unseen is not None else indices_us_i

            gaussians_xyz_unfrustum = self.gaussians_xyz[indices_unseen].detach().clone()
            gaussians_features_dc_unfrustum = self.gaussians_features_dc[indices_unseen].detach().clone()
            gaussians_features_rest_unfrustum = self.gaussians_features_rest[indices_unseen].detach().clone()
            gaussians_opacity_unfrustum = self.gaussians_opacity[indices_unseen].detach().clone()
            gaussians_scaling_unfrustum = self.gaussians_scaling[indices_unseen].detach().clone()
            gaussians_rotation_unfrustum = self.gaussians_rotation[indices_unseen].detach().clone()
            # [NEW] Split unseen attributes
            gaussians_creation_id_unfrustum = self.gaussians_creation_frame_id[indices_unseen].detach().clone()
            gaussians_ghost_count_unfrustum = self.gaussians_ghost_count[indices_unseen].detach().clone()

            # [NEW] Keep seen attributes
            self.gaussians_creation_frame_id = self.gaussians_creation_frame_id[indices].detach().clone()
            self.gaussians_ghost_count = self.gaussians_ghost_count[indices].detach().clone()
            self.gaussians_xyz_grad = self.gaussians_xyz[indices].detach().clone().requires_grad_(True)
            self.gaussians_features_dc_grad = self.gaussians_features_dc[indices].detach().clone().requires_grad_(True)
            self.gaussians_features_rest_grad = self.gaussians_features_rest[indices].detach().clone().requires_grad_(True)
            self.gaussians_opacity_grad = self.gaussians_opacity[indices].detach().clone().requires_grad_(True)
            self.gaussians_scaling_grad = self.gaussians_scaling[indices].detach().clone().requires_grad_(True)
            self.gaussians_rotation_grad = self.gaussians_rotation[indices].detach().clone().requires_grad_(True)
            masked_c_grad['indices'] = indices
        else:
            masked_c_grad = {}
            self.gaussians_xyz_grad = self.gaussians_xyz.detach().clone().requires_grad_(True)
            self.gaussians_features_dc_grad = self.gaussians_features_dc.detach().clone().requires_grad_(True)
            self.gaussians_features_rest_grad = self.gaussians_features_rest.detach().clone().requires_grad_(True)
            self.gaussians_opacity_grad = self.gaussians_opacity.detach().clone().requires_grad_(True)
            self.gaussians_scaling_grad = self.gaussians_scaling.detach().clone().requires_grad_(True)
            self.gaussians_rotation_grad = self.gaussians_rotation.detach().clone().requires_grad_(True)

        if self.encode_exposure:
            mlp_exposure_para_list += list(self.mlp_exposure.parameters())

        if self.BA:
            camera_tensor_list = []
            gt_camera_tensor_list = []
            for frame in optimize_frame:
                if frame != oldest_frame:
                    if frame != -1:
                        c2w = keyframe_dict[frame]['est_c2w']
                        gt_c2w = keyframe_dict[frame]['gt_c2w']
                    else:
                        c2w = cur_c2w
                        gt_c2w = gt_cur_c2w
                    camera_tensor = get_tensor_from_camera(c2w)
                    camera_tensor = Variable(
                        camera_tensor.to(device), requires_grad=True)
                    camera_tensor_list.append(camera_tensor)
                    gt_camera_tensor = get_tensor_from_camera(gt_c2w)
                    gt_camera_tensor_list.append(gt_camera_tensor)
        
        optim_para_list = [
            {'params': [self.gaussians_xyz_grad], 'lr': self.position_lr_init, "name": "xyz"},
            {'params': [self.gaussians_features_dc_grad], 'lr': self.feature_lr, "name": "f_dc"},
            {'params': [self.gaussians_features_rest_grad], 'lr': self.feature_lr / 20.0, "name": "f_rest"},
            {'params': [self.gaussians_opacity_grad], 'lr': self.opacity_lr, "name": "opacity"},
            {'params': [self.gaussians_scaling_grad], 'lr': self.scaling_lr, "name": "scaling"},
            {'params': [self.gaussians_rotation_grad], 'lr': self.rotation_lr, "name": "rotation"}
        ]

        if self.BA:
            optim_para_list.append({'params': camera_tensor_list, 'lr': 0, "name": "cam_tensor"})

        if self.encode_exposure:
            optim_para_list.append(
                {'params': self.exposure_feat, 'lr': 0.001, "name": "expos_feat"})
            optim_para_list.append(
                {'params': mlp_exposure_para_list, 'lr': 0.005, "name": "mlp_expos_para"})

        self.optimizer = torch.optim.Adam(optim_para_list)
        if idx > 0 and not color_refine:
            num_joint_iters = 100

        total_gs_point_num = self.gaussians.get_xyz().shape[0]
        print('total gaussian points number: ', f'{total_gs_point_num}')

        if self.frustum_feature_selection:
            frustum_gs_point_num = len(indices)
            print('cur window points number: ', f'{frustum_gs_point_num}')
            unseen_gs_point_num = len(indices_unseen)
            print('cur unseen window points number: ', f'{unseen_gs_point_num}')

        actual_joint_iters = 0

        for joint_iter in range(num_joint_iters):
            if joint_iter <= (self.geo_iter_first if init else int(num_joint_iters*self.geo_iter_ratio)):
                self.stage = 'geometry' 
            else:
                self.stage = 'color' 

            if self.BA:
                if joint_iter >= num_joint_iters*(self.geo_iter_ratio+0.2) and (joint_iter <= num_joint_iters*(self.geo_iter_ratio+0.6)):
                    self.optimizer.param_groups[6]['lr'] = self.BA_cam_lr
                else:
                    self.optimizer.param_groups[6]['lr'] = 0.0
            
            self.optimizer.zero_grad()

            images = []
            depths = []
            gt_colors = []
            gt_depths = []
            exposure_feat_list = []
            seg_masks = []
            camera_tensor_id = 0

            for frame in optimize_frame:
                if frame != -1:
                    gt_depth = keyframe_dict[frame]['depth']
                    gt_color = keyframe_dict[frame]['color']
                    seg_mask = keyframe_dict[frame]["seg_mask"]
                    if self.BA and frame != oldest_frame:
                        camera_tensor = camera_tensor_list[camera_tensor_id]
                        camera_tensor_id += 1
                        c2w = get_camera_from_tensor(camera_tensor)
                        c2w = torch.cat([c2w, bottom], dim=0)
                    else:
                        c2w = keyframe_dict[frame]['est_c2w']
                else:
                    gt_depth = cur_gt_depth
                    gt_color = cur_gt_color
                    seg_mask = cur_seg_mask
                    if self.BA:
                        camera_tensor = camera_tensor_list[camera_tensor_id]
                        c2w = get_camera_from_tensor(camera_tensor)
                        c2w = torch.cat([c2w, bottom], dim=0)
                    else:
                        c2w = cur_c2w

                if self.encode_exposure:
                    exposure_feat_list.append(
                        self.exposure_feat if frame == -1 else keyframe_dict[frame]['exposure_feat'])
                
                camera_center = c2w[:3,3]
                world_view_transform = torch.inverse(c2w).transpose(0,1)
                gaussians_opacity_grad_activation = self.opacity_activation(self.gaussians_opacity_grad)
                gaussians_scaling_grad_activation = self.scaling_activation(self.gaussians_scaling_grad)
                gaussians_rotation_grad_activation = self.rotation_activation(self.gaussians_rotation_grad)

                render_pkg = render(self.gaussians_xyz_grad, self.gaussians_features_dc_grad, self.gaussians_features_rest_grad,\
                                     gaussians_opacity_grad_activation, gaussians_scaling_grad_activation, gaussians_rotation_grad_activation,\
                                        self.gaussians.get_active_sh_degree(), self.gaussians.get_max_sh_degree(), camera_center, world_view_transform, self.projection_matrix,\
                                              self.fovx, self.fovy, self.H, self.W)

                image, viewspace_point_tensor, visibility_filter, radii = render_pkg["render"], render_pkg["viewspace_points"], render_pkg["visibility_filter"], render_pkg["radii"]
                depth = render_pkg["depth"][0]
                image = image.permute(1,2,0)
                
                if self.encode_exposure:
                    image = image.reshape(-1,3)
                    affine_tensor = self.mlp_exposure(exposure_feat_list[-1])
                    rot, trans = affine_tensor[:9].reshape(3, 3), affine_tensor[-3:]
                    image_slice = image.clone()
                    image_slice = torch.matmul(image_slice, rot) + trans
                    image = image_slice 
                    image = torch.sigmoid(image)
                    image = image.reshape(self.H, self.W, 3)

                images.append(image.unsqueeze(0))
                depths.append(depth.unsqueeze(0))
                gt_colors.append(gt_color.unsqueeze(0))
                gt_depths.append(gt_depth.unsqueeze(0))
                seg_masks.append(seg_mask.unsqueeze(0))

            if joint_iter % 30 == 0:
                self.gaussians.oneupSHdegree()

            images = torch.cat(images)
            depths = torch.cat(depths)
            gt_colors = torch.cat(gt_colors)
            gt_depths = torch.cat(gt_depths)
            seg_masks = torch.cat(seg_masks)

            mask = (gt_depths > 0.0) & (gt_depths < 8.) & seg_masks
            mask = mask & (~torch.isnan(gt_depths))
            depths_wmask = depths[mask]
            gt_depths_wmask = gt_depths[mask]

            # [æ–°å¢ä¿®æ”¹] åŒæ ·åº”ç”¨è·ç¦»æƒé‡è¡°å‡
            # è¿™é‡Œçš„ gt_depths_wmask åŒ…å«äº†æ‰€æœ‰é€‰å®šå¸§çš„æ‰€æœ‰æœ‰æ•ˆåƒç´ æ·±åº¦
            # é€»è¾‘å’Œ optimize_cur_map ä¸€æ ·ï¼šè¿œäº 4m çš„ç»™ä½æƒé‡
            dist_weights = torch.ones_like(gt_depths_wmask)
            dist_weights[gt_depths_wmask > 4.0] = 0.1 
            
            # åº”ç”¨æƒé‡è®¡ç®— Loss
            geo_loss = (torch.abs(gt_depths_wmask - depths_wmask) * dist_weights).sum()
            
            loss = geo_loss.clone() * self.w_geo_loss
            color_mask = mask
            color_loss = torch.abs(gt_colors[color_mask] - images[color_mask]).sum()

            for i in range(images.shape[0]):
                image = images[i]
                gt_color = gt_colors[i]
                ssim_loss = (1.0 - ssim(image.permute(2, 0, 1), gt_color.permute(2, 0, 1).float()))
                weighted_ssim_loss = self.lambda_ssim_loss*ssim_loss
                loss += weighted_ssim_loss
            weighted_color_loss = self.w_color_loss*color_loss*(1-self.lambda_ssim_loss)
            loss += weighted_color_loss

            loss.backward(retain_graph=False)

            self.max_radii2D = radii
            with torch.no_grad():
                if joint_iter > 0 and joint_iter % 20 == 0:
                    self.prune_neural_point(0.001)

                # [NEW] Call our new pruning logic
                # é¢‘ç‡ä¸ç”¨å¤ªé«˜ï¼Œæ¯ 50 æ¬¡è¿­ä»£æ£€æŸ¥ä¸€æ¬¡
                #if joint_iter > 0 and joint_iter % 50 == 0:
                #   self.prune_dynamic_ghosting(cur_c2w, cur_gt_depth, cur_gt_color, cur_seg_mask, idx.item(), age_thres=10)
                # åªåœ¨æœ€åä¸€æ¬¡è¿­ä»£è¿è¡Œï¼Œä¸”åªé’ˆå¯¹å¾ˆè€çš„ç‚¹ï¼ˆage_thres=20ï¼‰
                #if joint_iter == num_joint_iters - 1:
                    #self.prune_dynamic_ghosting(cur_c2w, cur_gt_depth, cur_gt_color, cur_seg_mask, idx.item(), age_thres=20)
                self.optimizer.step()
                self.optimizer.zero_grad()
                actual_joint_iters += 1
            
            total_gs_point_num_prune = self.gaussians_xyz.shape[0]
            if not self.wandb:
                if joint_iter % 100 == 0:
                    if self.stage == 'geometry':
                        print('iter: ', joint_iter, ', geo_loss: ', f'{geo_loss.item():0.6f}')
                    else:
                        print('iter: ', joint_iter, ', geo_loss: ', f'{geo_loss.item():0.6f}', ', color_loss: ', f'{color_loss.item():0.6f}')

            if joint_iter == num_joint_iters-1:
                print('idx: ', idx.item(), ', geo_loss_pixel: ', f'{(geo_loss.item()/mask.sum().item()):0.6f}',
                      ', color_loss_pixel: ', f'{(color_loss.item()/color_mask.sum().item()):0.4f}')
                if self.wandb:
                    wandb.log({'idx': int(idx.item()),
                                'geo_loss_pixel': float(f'{(geo_loss.item()/mask.sum().item()):0.6f}'),
                                'color_loss_pixel': float(f'{(color_loss.item()/color_mask.sum().item()):0.6f}'),
                                'pts_total': total_gs_point_num_prune})

                    wandb.log({'idx_map': int(idx.item()),
                               'num_joint_iters': num_joint_iters})
                    
        if self.frustum_feature_selection:
            indices = masked_c_grad['indices']
            if len(indices_unseen) == 0:
                self.gaussians_xyz[indices] = self.gaussians_xyz_grad.detach().clone()
                self.gaussians_features_dc[indices] = self.gaussians_features_dc_grad.detach().clone()
                self.gaussians_features_rest[indices] = self.gaussians_features_rest_grad.detach().clone()
                self.gaussians_opacity[indices] = self.gaussians_opacity_grad.detach().clone()
                self.gaussians_scaling[indices] = self.gaussians_scaling_grad.detach().clone()
                self.gaussians_rotation[indices] = self.gaussians_rotation_grad.detach().clone()
            else:
                self.gaussians_xyz = torch.cat((self.gaussians_xyz_grad.detach().clone(), gaussians_xyz_unfrustum.detach().clone()), 0)
                self.gaussians_features_dc = torch.cat((self.gaussians_features_dc_grad.detach().clone(), gaussians_features_dc_unfrustum.detach().clone()), 0)
                self.gaussians_features_rest = torch.cat((self.gaussians_features_rest_grad.detach().clone(), gaussians_features_rest_unfrustum.detach().clone()), 0)
                self.gaussians_opacity = torch.cat((self.gaussians_opacity_grad.detach().clone(), gaussians_opacity_unfrustum.detach().clone()), 0)
                self.gaussians_scaling = torch.cat((self.gaussians_scaling_grad.detach().clone(), gaussians_scaling_unfrustum.detach().clone()), 0)
                self.gaussians_rotation = torch.cat((self.gaussians_rotation_grad.detach().clone(), gaussians_rotation_unfrustum.detach().clone()), 0)
                # [NEW] Cat attributes
                self.gaussians_creation_frame_id = torch.cat((self.gaussians_creation_frame_id, gaussians_creation_id_unfrustum), 0)
                self.gaussians_ghost_count = torch.cat((self.gaussians_ghost_count, gaussians_ghost_count_unfrustum), 0)
                masked_c_grad['indices'] = np.arange(self.gaussians_xyz_grad.shape[0]).tolist()
                indices_unseen = np.arange(self.gaussians_xyz_grad.shape[0], self.gaussians_xyz_grad.shape[0] + gaussians_xyz_unfrustum.shape[0]).tolist()
        else:
            self.gaussians_xyz = self.gaussians_xyz_grad.detach().clone()
            self.gaussians_features_dc = self.gaussians_features_dc_grad.detach().clone()
            self.gaussians_features_rest = self.gaussians_features_rest_grad.detach().clone()
            self.gaussians_opacity = self.gaussians_opacity_grad.detach().clone()
            self.gaussians_scaling = self.gaussians_scaling_grad.detach().clone()
            self.gaussians_rotation = self.gaussians_rotation_grad.detach().clone()

        if self.frustum_feature_selection:
            self.gaussians.update_xyz(self.gaussians_xyz.detach().clone())
            self.gaussians.update_features_dc(self.gaussians_features_dc.detach().clone())
            self.gaussians.update_features_rest(self.gaussians_features_rest.detach().clone())
            self.gaussians.update_scaling(self.gaussians_scaling.detach().clone())
            self.gaussians.update_rotation(self.gaussians_rotation.detach().clone())
            self.gaussians.update_opacity(self.gaussians_opacity.detach().clone())
            # [NEW] Update model attributes
            self.gaussians.update_creation_frame_id(self.gaussians_creation_frame_id.detach().clone())
            self.gaussians.update_ghost_count(self.gaussians_ghost_count.detach().clone())
        else:
            self.gaussians.update_xyz(self.gaussians_xyz.detach().clone())
            self.gaussians.update_features_dc(self.gaussians_features_dc.detach().clone())
            self.gaussians.update_features_rest(self.gaussians_features_rest.detach().clone())
            self.gaussians.update_scaling(self.gaussians_scaling.detach().clone())
            self.gaussians.update_rotation(self.gaussians_rotation.detach().clone())
            self.gaussians.update_opacity(self.gaussians_opacity.detach().clone())
            # [NEW] Update model attributes
            self.gaussians.update_creation_frame_id(self.gaussians_creation_frame_id.detach().clone())
            self.gaussians.update_ghost_count(self.gaussians_ghost_count.detach().clone())
        print('Mapper has updated point features.')

        if self.BA:
            camera_tensor_id = 0
            for id, frame in enumerate(optimize_frame):
                if frame != -1:
                    if frame != oldest_frame:
                        c2w = get_camera_from_tensor(
                            camera_tensor_list[camera_tensor_id].detach())
                        c2w = torch.cat([c2w, bottom], dim=0)
                        camera_tensor_id += 1
                        keyframe_dict[frame]['est_c2w'] = c2w.clone()
                else:
                    c2w = get_camera_from_tensor(
                        camera_tensor_list[-1].detach())
                    c2w = torch.cat([c2w, bottom], dim=0)
                    cur_c2w = c2w.clone()
            print('Mapper has updated optimize pose (BA).')

        if self.encode_exposure and idx == (self.n_img-1):
            self.exposure_feat_all.append(self.exposure_feat.detach().cpu())
        if self.BA:
            return cur_c2w
        else:
            return None

    def update_para_from_mapping(self):
        """
        Update the parameters of scene representation from the mapping thread.

        """
        if self.mapping_idx[0] != self.prev_mapping_idx:
            self.gaussians_xyz = self.gaussians.get_xyz().detach().clone()
            self.gaussians_features_dc = self.gaussians.get_features_dc().detach().clone()
            self.gaussians_features_rest = self.gaussians.get_features_rest().detach().clone()
            self.gaussians_opacity = self.gaussians.get_opacity().detach().clone()
            self.gaussians_scaling = self.gaussians.get_scaling().detach().clone()
            self.gaussians_rotation = self.gaussians.get_rotation().detach().clone()
            self.prev_mapping_idx = self.mapping_idx[0].clone()
            if self.verbose:
                print('Tracker has updated the parameters from Mapper.')

    def optimize_cam_in_batch(self, camera_tensor, gt_color, gt_depth, batch_size, optimizer, seg_mask,
                              selected_index=None):
        """
        Do one iteration of camera iteration. Sample pixels, render depth/color, calculate loss and backpropagation.

        Args:
            camera_tensor (tensor): camera tensor.
            gt_color (tensor): ground truth color image of the current frame.
            gt_depth (tensor): ground truth depth image of the current frame.
            batch_size (int): batch size, number of sampling rays.
            optimizer (torch.optim): camera optimizer.
            selected_index: top color gradients pixels are pre-selected.

        Returns:
            loss (float): total loss
            color_loss (float): color loss component
            geo_loss (float): geometric loss component
        """
        H, W = self.H, self.W
        optimizer.zero_grad()
        c2w = get_camera_from_tensor(camera_tensor)
        Wedge = self.ignore_edge_W
        Hedge = self.ignore_edge_H

        with torch.no_grad():
            inside_mask = gt_depth <= torch.minimum(
                10*gt_depth.median(), 1.2*torch.max(gt_depth))
            edge_mask = torch.zeros((H, W), dtype=torch.bool)
            edge_mask = edge_mask.to(inside_mask)
            edge_mask[Hedge:H-Hedge, Wedge:W-Wedge] = True

        camera_center = c2w[:3,3]
        world_view_transform = torch.inverse(convert3x4_4x4(c2w)).transpose(0,1)
        gaussians_opacity_activation = self.opacity_activation(self.gaussians_opacity)
        gaussians_scaling_activation = self.scaling_activation(self.gaussians_scaling)
        gaussians_rotation_activation = self.rotation_activation(self.gaussians_rotation)
        render_pkg = render(self.gaussians_xyz, self.gaussians_features_dc, self.gaussians_features_rest,\
                                gaussians_opacity_activation, gaussians_scaling_activation, gaussians_rotation_activation,\
                                    self.gaussians.get_active_sh_degree(), self.gaussians.get_max_sh_degree(), camera_center, world_view_transform,\
                                        self.projection_matrix, self.fovx, self.fovy, H, W)
       
        image, viewspace_point_tensor, visibility_filter, radii = render_pkg["render"], render_pkg["viewspace_points"], render_pkg["visibility_filter"], render_pkg["radii"]
        depth = render_pkg["depth"][0]
        image = image.permute(1,2,0)
        opacity_acc = render_pkg["acc"]
        opacity_mask = opacity_acc > self.opacity_thres
        opacity_mask = opacity_mask.squeeze()

        if self.encode_exposure:
            image = image.reshape(-1,3)
            affine_tensor = self.mlp_exposure(self.exposure_feat)
            rot, trans = affine_tensor[:9].reshape(3, 3), affine_tensor[-3:]
            image_slice = image.clone()
            image_slice = torch.matmul(image_slice, rot) + trans
            image = image_slice
            image = torch.sigmoid(image)
            image = image.reshape(self.H, self.W, 3)

        nan_mask = (~torch.isnan(depth))
        if self.ignore_outlier_depth_loss:
            depth_error = torch.abs(gt_depth-depth) * (gt_depth > 0)
            mask = (depth_error < 10*(depth_error[opacity_mask].median()))
            mask = mask & (depth > 0) & (gt_depth > 0) & (gt_depth < 8.0)
        else:
            mask = (gt_depth > 0) & (gt_depth < 8.0)

        mask = mask & nan_mask
        mask = mask & edge_mask
        mask = mask & seg_mask

        if self.use_opacity_mask_for_loss:
            mask = mask & opacity_mask

        geo_loss = torch.clamp((torch.abs(gt_depth-depth)), min=0.0, max=1e3)[mask].sum()
        loss = self.w_geo_loss_tracking * geo_loss
        color_loss = torch.abs(gt_color - image)[mask].sum()
        if self.use_color_in_tracking:
            loss += self.w_color_loss_tracking*color_loss
        
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        self.opacity_mask = opacity_mask
        return loss.item(), (color_loss/mask.shape[0]).item(), (geo_loss/mask.shape[0]).item(), image, depth, mask
    
    def convert_relative_pose(self, idx):
        poses = torch.zeros((idx+1, 4, 4))
        for i in range(idx+1):
            if i % self.keyframe_every == 0:
                poses[i] = self.estimate_c2w_rel_list[i]
            else:
                kf_id = i // self.keyframe_every
                kf_frame_id = kf_id * self.keyframe_every
                c2w_key = self.estimate_keyframe_dict[kf_frame_id]
                delta = self.estimate_c2w_rel_list[i]
                poses[i] = delta @ c2w_key
        return poses

    def tracking(self, idx, gt_color, gt_depth, gt_c2w, estimated_c2w, seg_mask):
        device = self.device
        self.update_para_from_mapping()

        if self.verbose:
            print(Fore.MAGENTA)
            print("Tracking KeyFrame ",  idx.item())
            print(Style.RESET_ALL)

        if idx <= 1:
            c2w = gt_c2w
        else:
            self.count_bound = (idx % 5) if (idx % 5) > 0 else 5
            gt_camera_tensor = get_tensor_from_camera(gt_c2w)
            self.num_cam_iters = self.cfg['tracking']['iters']
            estimated_new_cam_c2w = estimated_c2w
            camera_tensor = get_tensor_from_camera(
                estimated_new_cam_c2w.detach())
            
            if torch.dot(camera_tensor[:4], gt_camera_tensor[:4]).item() < 0:
                camera_tensor[:4] *= -1

            if self.separate_LR:
                camera_tensor = camera_tensor.to(device).detach()
                T = camera_tensor[-3:]
                quad = camera_tensor[:4]
                self.quad = Variable(quad, requires_grad=True)
                self.T = Variable(T, requires_grad=True)
                camera_tensor = torch.cat([quad, T], 0)
                cam_para_list_T = [self.T]
                cam_para_list_quad = [self.quad]
                optim_para_list = [{'params': cam_para_list_T, 'lr': self.cam_lr},
                                    {'params': cam_para_list_quad, 'lr': self.cam_lr*0.4}]
            else:
                camera_tensor = Variable(
                    camera_tensor.to(device), requires_grad=True)
                cam_para_list = [camera_tensor]
                optim_para_list = [
                    {'params': cam_para_list, 'lr': self.cam_lr}]
                
            optimizer_camera = torch.optim.Adam(optim_para_list)

            candidate_cam_tensor = None
            current_min_loss = float(1e20)

            actual_cam_iters = 0
            for cam_iter in range(self.num_cam_iters):
                actual_cam_iters +=1
                if self.separate_LR:
                    camera_tensor = torch.cat(
                        [self.quad, self.T], 0).to(self.device)

                loss, color_loss_pixel, geo_loss_pixel, image, depth, mask =\
                        self.optimize_cam_in_batch(camera_tensor, gt_color, gt_depth, self.tracking_pixels, optimizer_camera, seg_mask)

                if cam_iter == 0:
                    initial_loss = loss

                loss_camera_tensor = torch.abs(
                    gt_camera_tensor.to(device)-camera_tensor).mean().item()
                if loss <= current_min_loss:
                    current_min_loss = loss
                    candidate_cam_tensor = camera_tensor.detach().clone()
                if cam_iter == self.num_cam_iters-1:
                    if not self.wandb:
                        print(f'idx:{idx}, re-rendering loss: {initial_loss:.2f}->{current_min_loss:.2f}.')
               
                if (cam_iter + 1) % 20 == 0 or cam_iter == 0:
                    if not self.wandb:
                        print(f'iter: {cam_iter}, camera tensor error: {loss_camera_tensor:.4f},  tracking loss: {loss:.4f}')

            bottom = torch.tensor(
                [0, 0, 0, 1.0], device=self.device).reshape(1, 4)
            c2w = get_camera_from_tensor(
                candidate_cam_tensor.detach().clone())
            c2w = torch.cat([c2w, bottom], dim=0)
            print(f'Finish tracking opt, plan iter: {self.num_cam_iters}, actual opt iter: {actual_cam_iters}')

        self.estimate_c2w_list[idx] = c2w.clone().cpu()
        self.gt_c2w_list[idx] = gt_c2w.clone().cpu()
        self.pre_c2w = c2w.clone()
        self.idx[0] = idx

        if self.low_gpu_mem:
            torch.cuda.empty_cache()
        if self.wandb:
            wandb.finish()

    def mapping(self, idx, gt_color, gt_depth, gt_c2w, estimated_c2w, seg_mask, init = False): 
        print(f"    -> [DEBUG GS] mapping(). idx={idx}")       
        if self.verbose:
            print(Fore.GREEN)
            print("Mapping KeyFrame ", idx.item())
            print(Style.RESET_ALL)
        
        color_refine = True if (idx == self.n_img-1 and self.color_refine) else False

        if not init:                                                                                                                                                                 
            num_joint_iters = self.cfg['mapping']['iters']
            self.mapping_window_size = self.cfg['mapping']['mapping_window_size']*(
                2 if self.n_img > 8000 else 1)
            if idx == self.n_img-1 and self.color_refine:
                outer_joint_iters = 1
                self.mapping_window_size *= 2
                self.geo_iter_ratio = 0.0
                num_joint_iters *= 2
                self.fix_color_decoder = True
                self.frustum_feature_selection = False
                self.keyframe_selection_method = 'global'
            else:
                outer_joint_iters = 1
        else:
            outer_joint_iters = 1
            num_joint_iters = self.iters_first

        cur_c2w = self.estimate_c2w_list[idx].to(self.device)
        for outer_joint_iter in range(outer_joint_iters):
            self.BA = (len(self.keyframe_list) >
                        4) and self.cfg['mapping']['BA']
            self.optimize_cur_map(num_joint_iters, idx, gt_color, gt_depth, gt_c2w, cur_c2w, color_refine=color_refine, seg_mask = seg_mask)
            _ = self.optimize_map(num_joint_iters, idx, gt_color, gt_depth, gt_c2w,
                                    self.keyframe_dict, self.keyframe_list, cur_c2w, color_refine=color_refine, cur_seg_mask = seg_mask)
            if self.BA:
                cur_c2w = _
                self.estimate_c2w_list[idx] = cur_c2w
                

        if (idx % self.keyframe_every == 0 or idx == self.n_img - 2) and \
           (idx not in self.keyframe_list) and \
           (not torch.isinf(gt_c2w).any()) and (not torch.isnan(gt_c2w).any()):  
            self.keyframe_list.append(idx)
            self.last_keyframe_idx = idx # æ›´æ–°æœ€åå…³é”®å¸§ID

            dic_of_cur_frame = {'gt_c2w': gt_c2w.detach(), 'idx': idx, 'color': gt_color.detach(),
                                'depth': gt_depth.detach(), 'est_c2w': cur_c2w.detach().clone(), "seg_mask": seg_mask.detach()}
            if self.use_dynamic_radius:
                dic_of_cur_frame.update(
                    {'dynamic_r_query': self.dynamic_r_query.detach()})
            if self.encode_exposure:
                dic_of_cur_frame.update(
                    {'exposure_feat': self.exposure_feat.detach()})
            self.keyframe_dict.append(dic_of_cur_frame)
            # --- æ–°å¢ä»£ç ï¼šåœ¨æ­¤å¤„è¿›è¡Œæ¸²æŸ“å¹¶ä¿å­˜ ---
            with torch.no_grad():
                # å‡†å¤‡æ¸²æŸ“å‚æ•°
                camera_center = cur_c2w[:3, 3]
                world_view_transform = torch.inverse(cur_c2w).transpose(0, 1)
                
                # è·å–å½“å‰æ‰€æœ‰é«˜æ–¯çš„æ¿€æ´»æ€å‚æ•°
                gaussians_opacity = self.opacity_activation(self.gaussians.get_opacity())
                gaussians_scaling = self.scaling_activation(self.gaussians.get_scaling())
                gaussians_rotation = self.rotation_activation(self.gaussians.get_rotation())
                
                # è°ƒç”¨æ¸²æŸ“å™¨
                render_pkg = render(
                    self.gaussians.get_xyz(), 
                    self.gaussians.get_features_dc(), 
                    self.gaussians.get_features_rest(),
                    gaussians_opacity, 
                    gaussians_scaling, 
                    gaussians_rotation,
                    self.gaussians.get_active_sh_degree(), 
                    self.gaussians.get_max_sh_degree(),
                    camera_center, 
                    world_view_transform, 
                    self.projection_matrix, 
                    self.fovx, 
                    self.fovy, 
                    self.H, 
                    self.W
                )
                
                # å¤„ç†æ¸²æŸ“å‡ºçš„å›¾åƒ
                image_render = render_pkg["render"] # [3, H, W]
                image_np = image_render.permute(1, 2, 0).cpu().numpy()
                image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)
                image_bgr = cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)
                
                # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨å¹¶ä¿å­˜
                render_save_dir = os.path.join(self.output, 'render_frames')
                os.makedirs(render_save_dir, exist_ok=True)
                save_path = os.path.join(render_save_dir, f'frame_{idx:05d}.png')
                cv2.imwrite(save_path, image_bgr)
                print(f'Saved rendering for keyframe {idx} to {save_path}')
            # --- æ–°å¢ä»£ç ç»“æŸ ---
        
        self.pre_c2w = self.estimate_c2w_list[idx].to(self.device)

        if (idx > 0 and idx % self.ckpt_freq == 0) or idx == self.n_img-1:
            self.logger.log(idx, self.keyframe_dict, self.keyframe_list,
                            selected_keyframes=self.selected_keyframes
                            if self.save_selected_keyframes_info else None, gaussians=self.gaussians,
                            exposure_feat=self.exposure_feat_all
                            if self.encode_exposure else None)
        if idx == self.n_img-1:
            print('Color refinement done.')
            print('Mapper finished.')
        if self.low_gpu_mem:
            torch.cuda.empty_cache() 

    def run(self, idx, gt_color, gt_depth, gt_c2w, estimated_c2w, seg_mask): 
        """
        Dispatch Threads. # this func, when called, act as main process
        """
        print(f"  -> [DEBUG GS] run() called. Passed idx={idx}")
        gt_color = gt_color.permute(1,2,0)

        if self.edge is not None:
            seg_mask = seg_mask[self.edge:-self.edge, self.edge:-self.edge]
            gt_color = gt_color[self.edge:-self.edge, self.edge:-self.edge]
            gt_depth = gt_depth[self.edge:-self.edge, self.edge:-self.edge]
        if idx == 0:
            init = True
        else:
            init = False

        if self.use_dynamic_radius:
            ratio = self.radius_query_ratio
            intensity = rgb2gray(gt_color.cpu().numpy())
            grad_y = filters.sobel_h(intensity)
            grad_x = filters.sobel_v(intensity)
            color_grad_mag = np.sqrt(grad_x**2 + grad_y**2)
            color_grad_mag = np.clip(
                color_grad_mag, 0.0, self.color_grad_threshold)
            if idx > 0:
                fn_map_r_add = interp1d([0, 0.01, self.color_grad_threshold], [
                    self.radius_add_max, self.radius_add_max, self.radius_add_min])
                fn_map_r_query = interp1d([0, 0.01, self.color_grad_threshold], [
                    ratio * self.radius_add_max, ratio * self.radius_add_max, ratio * self.radius_add_min])
            else:
                fn_map_r_add = interp1d([0, 0.01, self.color_grad_threshold], [
                    self.radius_add_max / 2, self.radius_add_max / 2, self.radius_add_min / 2])
                fn_map_r_query = interp1d([0, 0.01, self.color_grad_threshold], [ratio * self.radius_add_max / 2, ratio * self.radius_add_max / 2,
                    ratio * self.radius_add_min / 2])

            dynamic_r_add = fn_map_r_add(color_grad_mag)
            dynamic_r_query = fn_map_r_query(color_grad_mag)
            self.dynamic_r_add, self.dynamic_r_query = torch.from_numpy(dynamic_r_add).to(
                self.device), torch.from_numpy(dynamic_r_query).to(self.device)
        
        if idx == 0:
            self.estimate_c2w_list[0] = gt_c2w.cpu()
            self.gt_c2w_list[0] = gt_c2w.cpu()
            self.mapping(idx, gt_color, gt_depth, gt_c2w, estimated_c2w, seg_mask,  init)
            self.tracking(idx, gt_color, gt_depth, gt_c2w, estimated_c2w, seg_mask)

        else:
            self.tracking(idx, gt_color, gt_depth, gt_c2w, estimated_c2w, seg_mask)
            self.mapping(idx, gt_color, gt_depth, gt_c2w, estimated_c2w, seg_mask, init)
